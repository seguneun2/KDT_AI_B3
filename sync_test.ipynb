{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get DIR information.\n",
      "프로그램 실행 시간: 5.08초\n",
      "API call 횟수 : 19\n",
      "OPEN AI deeplake 에 업로드 시작..Deep Lake Dataset in hub://seungeunlee/langchain-code already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://seungeunlee/langchain-code', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (267, 1536)  float32   None   \n",
      "    id        text      (267, 1)      str     None   \n",
      " metadata     json      (267, 1)      str     None   \n",
      "   text       text      (267, 1)      str     None   \n",
      "Deep Lake Dataset in hub://seungeunlee/langchain-code already exists, loading from the storage\n",
      "  완료! \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from validation import url_check\n",
    "from get_info_from_api import github_api_call\n",
    "from data_processing import (\n",
    "    document_processing,\n",
    "    document_chunking,\n",
    ")\n",
    "from vector_db import upload_document, get_retriever\n",
    "from question import get_conversation_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "github_link = input(\"GitHub 링크를 입력해주세요 : \")\n",
    "github_info_dict = github_api_call(github_link, make_txt_file=False)\n",
    "    \n",
    "# 3. chunking 후 text processing 하기 \n",
    "docs = document_processing(github_info_dict, make_txt_file=False)\n",
    "texts = document_chunking(docs, size=1000, overlap=0)\n",
    "    \n",
    "# 4. chunking 된 데이터 vector db 로 임베딩 하기 \n",
    "db = upload_document(texts, os.getenv('DEEPLAKE_USERNAME'), 'langchain-code')\n",
    "retriever =  get_retriever(db, \"cos\", 100, True, 10)\n",
    "    \n",
    "# 5. 원하는 질문 입력 하기 \n",
    "model, qa = get_conversation_chain(\"gpt-3.5-turbo\", retriever)\n",
    "    \n",
    "# 6. QA 시작\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_qu(question, chat_history):\n",
    "    try:\n",
    "        await asyncio.wait_for(qa({\"question\": question, \"chat_history\": chat_history}), timeout=2.0)\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Time out. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "question_list = [\n",
    "    'what is Transformer?',\n",
    "    'What is role of encoder?',\n",
    "    'Please explain Decoder\\'s code'\n",
    "]\n",
    "\n",
    "# for question in question_list:\n",
    "#     result = get_qu(question, chat_history)\n",
    "#     chat_history.append((question, result[\"answer\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what is Transformer?',\n",
       "  'Transformer is a model architecture that is widely used in natural language processing tasks, such as machine translation, text generation, and text classification. It was introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. \\n\\nThe Transformer model is based on a self-attention mechanism, which allows the model to focus on different parts of the input sequence during processing. This attention mechanism helps the model capture long-range dependencies and improves its performance on tasks that require understanding of context.\\n\\nThe Transformer model has achieved state-of-the-art results in many NLP tasks and has become a cornerstone in the field of deep learning for natural language processing.'),\n",
       " ('What is role of encoder?',\n",
       "  'The role of the encoder in the Transformer model is to process the input sequence and generate a representation that captures the semantic meaning of the sequence. The encoder consists of multiple layers, each containing self-attention and feed-forward neural network modules. The self-attention mechanism allows the encoder to attend to different parts of the input sequence, capturing both local and global dependencies. The feed-forward neural network module then applies non-linear transformations to the input representation. By stacking multiple layers, the encoder can capture complex patterns in the input sequence and generate a rich representation that is fed into the decoder for further processing.'),\n",
       " (\"Please explain Decoder's code\",\n",
       "  'The Decoder in the Transformer model is responsible for generating the output sequence based on the encoded input sequence. Here is a breakdown of the code:\\n\\n1. The `Decoder` class is defined as a subclass of `nn.Module`.\\n\\n2. The constructor method `__init__` initializes the parameters of the decoder. It takes arguments `vocab_size` (size of the vocabulary), `d_model` (dimension of the model), `N` (number of layers), `heads` (number of attention heads), and `dropout` (dropout rate). \\n\\n3. Inside the constructor, the decoder initializes an embedding layer `embed` using the `Embedder` class, which maps the input tokens to a continuous vector space of dimension `d_model`. \\n\\n4. The decoder also initializes a positional encoding layer `pe` using the `PositionalEncoder` class, which adds positional information to the embedded input tokens.\\n\\n5. The decoder creates `N` layers of `DecoderLayer` using the `get_clones` function. Each `DecoderLayer` applies self-attention and cross-attention to the input sequence.\\n\\n6. The decoder defines a normalization layer `norm` using the `Norm` class, which applies layer normalization to the output of the decoder layers.\\n\\n7. The `forward` method takes input arguments `trg` (target sequence), `e_outputs` (encoded input sequence), `src_mask` (mask for the source sequence), and `trg_mask` (mask for the target sequence).\\n\\n8. Inside the `forward` method, the input target sequence `trg` is passed through the embedding layer `embed` and the positional encoding layer `pe`.\\n\\n9. The decoder iterates over the `N` decoder layers and applies each layer to the target sequence `trg`, the encoded input sequence `e_outputs`, and the masks `src_mask` and `trg_mask`.\\n\\n10. The output of the decoder layers is passed through the normalization layer `norm`.\\n\\n11. The normalized output is returned as the final output of the decoder.\\n\\nOverall, the Decoder in the Transformer model takes the target sequence, the encoded input sequence, and masks as input, and generates the output sequence by applying multiple layers of self-attention and cross-attention.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import functools\n",
    "\n",
    "def timeout(timeout):\n",
    "    def deco(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            res = [Exception('function timeout [%s seconds] exceeded!' % (timeout))]\n",
    "            def newFunc():\n",
    "                try:\n",
    "                    res[0] = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    res[0] = e\n",
    "            t = Thread(target=newFunc)\n",
    "            t.daemon = True\n",
    "            try:\n",
    "                t.start()\n",
    "                t.join(timeout)\n",
    "            except Exception as je:\n",
    "                print ('error starting thread')\n",
    "                raise je\n",
    "            ret = res[0]\n",
    "            if isinstance(ret, BaseException):\n",
    "                raise ret\n",
    "            return ret\n",
    "        return wrapper\n",
    "    return deco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function timeout [<function timeout at 0x00000189941B6480> seconds] exceeded!\n"
     ]
    }
   ],
   "source": [
    "res = [Exception('function timeout [%s seconds] exceeded!' % (timeout))]\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "func = timeout(timeout=5)(qa)\n",
    "\n",
    "\n",
    "for question in question_list:\n",
    "    try : \n",
    "        result = func(({\"question\": question, \"chat_history\": chat_history}))\n",
    "        chat_history.append((question, result[\"answer\"]))\n",
    "    except Exception as e:\n",
    "        pass #print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Please explain Decoder's code\",\n",
       "  \"I'm sorry, but I cannot see the actual code for the Decoder in the provided context. The context only shows some cloning and licensing information related to a repository. If you could provide the actual code for the Decoder or give more specific information, I would be happy to help explain it.\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Batch.py', metadata={'content': 'import torch\\nfrom torchtext.legacy import data\\nimport numpy as np\\nfrom torch.autograd import Variable\\n\\n\\ndef nopeak_mask(size, opt):\\n    np_mask = np.triu(np.ones((1, size, size)), k=1).astype(\\'uint8\\')\\n    np_mask = Variable(torch.from_numpy(np_mask == 0).to(opt.device))\\n    return np_mask\\n\\ndef create_masks(src, trg, opt):\\n    \\n    src_mask = (src != opt.src_pad).unsqueeze(-2).to(opt.device)\\n\\n    if trg is not None:\\n        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(opt.device)\\n        size = trg.size(1)  # get seq_len for matrix\\n        np_mask = nopeak_mask(size, opt).to(opt.device)\\n        trg_mask = trg_mask & np_mask\\n\\n    else:\\n        trg_mask = None\\n    return src_mask, trg_mask\\n\\n# patch on Torchtext\\'s batching process that makes it more efficient\\n# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\\n\\nclass MyIterator(data.Iterator):\\n    def create_batches(self):\\n        if self.train:\\n            def pool(d, random_shuffler):\\n                for p in data.batch(d, self.batch_size * 100):\\n                    p_batch = data.batch(\\n                        sorted(p, key=self.sort_key),\\n                        self.batch_size, self.batch_size_fn)\\n                    for b in random_shuffler(list(p_batch)):\\n                        yield b\\n            self.batches = pool(self.data(), self.random_shuffler)\\n            \\n        else:\\n            self.batches = []\\n            for b in data.batch(self.data(), self.batch_size,\\n                                          self.batch_size_fn):\\n                self.batches.append(sorted(b, key=self.sort_key))\\n\\nglobal max_src_in_batch, max_tgt_in_batch\\n\\ndef batch_size_fn(new, count, sofar):\\n    \"Keep augmenting batch and calculate total number of tokens + padding.\"\\n    global max_src_in_batch, max_tgt_in_batch\\n    if count == 1:\\n        max_src_in_batch = 0\\n        max_tgt_in_batch = 0\\n    max_src_in_batch = max(max_src_in_batch,  len(new.src))\\n    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\\n    src_elements = count * max_src_in_batch\\n    tgt_elements = count * max_tgt_in_batch\\n    return max(src_elements, tgt_elements)\\n'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
